{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc38e78b",
   "metadata": {},
   "source": [
    "# Classe NLP Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9dd08b",
   "metadata": {},
   "source": [
    "## Caricamento delle librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1db5c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "import dateparser\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6ba3891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"it_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f834070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c475d2",
   "metadata": {},
   "source": [
    "## Alcune funzioni utili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91f315ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"def remove_nan(df: pd.DataFrame) -> dict:\\n\\n    \\\"\\\"\\\"Rimuove i valori nulli da una lista\\\"\\\"\\\"\\n\\n    lookup_dict = df.to_dict(\\\"list\\\")\\n\\n    for k, v in lookup_dict.items():\\n\\n        while np.nan in lookup_dict[k]:\\n            lookup_dict[k].remove(np.nan)\\n\\n    return lookup_dict\";\n",
       "                var nbb_formatted_code = \"def remove_nan(df: pd.DataFrame) -> dict:\\n\\n    \\\"\\\"\\\"Rimuove i valori nulli da una lista\\\"\\\"\\\"\\n\\n    lookup_dict = df.to_dict(\\\"list\\\")\\n\\n    for k, v in lookup_dict.items():\\n\\n        while np.nan in lookup_dict[k]:\\n            lookup_dict[k].remove(np.nan)\\n\\n    return lookup_dict\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def remove_nan(df: pd.DataFrame) -> dict:\n",
    "\n",
    "    \"\"\"Rimuove i valori nulli da una lista\"\"\"\n",
    "\n",
    "    lookup_dict = df.to_dict(\"list\")\n",
    "\n",
    "    for k, v in lookup_dict.items():\n",
    "\n",
    "        while np.nan in lookup_dict[k]:\n",
    "            lookup_dict[k].remove(np.nan)\n",
    "\n",
    "    return lookup_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a49f5ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rimuovi_punteggiatura(text):\n",
    "\n",
    "    \"\"\"\n",
    "    I dati di training contengono parentesi quadre e tonde,\n",
    "        quindi non vanno eliminate in questa fase\n",
    "    \"\"\"\n",
    "\n",
    "    text = re.sub(r\"[?]\", \" ?\", text)\n",
    "    text = re.sub(r\"[\\.,;:!]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b68a84e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"def spacy_entities_extractor(txt, npl: spacy):\\n\\n    doc = nlp(txt.capitalize())\\n    token = doc[0]\\n\\n    if token.ent_type_ != \\\"\\\":\\n        return token.ent_type_  # pos_\\n    else:\\n        return \\\"O\\\"\";\n",
       "                var nbb_formatted_code = \"def spacy_entities_extractor(txt, npl: spacy):\\n\\n    doc = nlp(txt.capitalize())\\n    token = doc[0]\\n\\n    if token.ent_type_ != \\\"\\\":\\n        return token.ent_type_  # pos_\\n    else:\\n        return \\\"O\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def spacy_entities_extractor(txt, npl: spacy):\n",
    "\n",
    "    doc = nlp(txt.capitalize())\n",
    "    token = doc[0]\n",
    "\n",
    "    if token.ent_type_ != \"\":\n",
    "        return token.ent_type_  # pos_\n",
    "    else:\n",
    "        return \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d560640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"def extend_data(df: pd.DataFrame, spacy: bool = False) -> pd.DataFrame:\\n\\n    \\\"\\\"\\\"\\n    Estende i dati attraverso un algortimo personalizzato\\n    \\\"\\\"\\\"\\n\\n    df[\\\"shift-3\\\"] = df.groupby(\\\"n_frase\\\")[\\\"word\\\"].shift(1).str.slice(-3)\\n    df[\\\"shift+3\\\"] = df.groupby(\\\"n_frase\\\")[\\\"word\\\"].shift(-1).str.slice(0, 3)\\n    df[\\\"shift-10\\\"] = df.groupby(\\\"n_frase\\\")[\\\"word\\\"].shift(1).str.slice(-10)\\n    df[\\\"shift+10\\\"] = df.groupby(\\\"n_frase\\\")[\\\"word\\\"].shift(-1).str.slice(0, 10)\\n\\n    df[\\\"shift-3\\\"].fillna(\\\"BOF\\\", inplace=True)\\n    df[\\\"shift+3\\\"].fillna(\\\"EOF\\\", inplace=True)\\n    df[\\\"shift-10\\\"].fillna(\\\"BOF\\\", inplace=True)\\n    df[\\\"shift+10\\\"].fillna(\\\"EOF\\\", inplace=True)\\n\\n    if spacy:\\n        df[\\\"spacy\\\"] = df[\\\"word\\\"].apply(spacy_entities_extractor)\\n\\n    df[\\\"bias\\\"] = 1\\n\\n    try:\\n        df.drop(columns=[\\\"tag\\\"], inplace=True)\\n    except:\\n        pass\\n\\n    return df\";\n",
       "                var nbb_formatted_code = \"def extend_data(df: pd.DataFrame, spacy: bool = False) -> pd.DataFrame:\\n\\n    \\\"\\\"\\\"\\n    Estende i dati attraverso un algortimo personalizzato\\n    \\\"\\\"\\\"\\n\\n    df[\\\"shift-3\\\"] = df.groupby(\\\"n_frase\\\")[\\\"word\\\"].shift(1).str.slice(-3)\\n    df[\\\"shift+3\\\"] = df.groupby(\\\"n_frase\\\")[\\\"word\\\"].shift(-1).str.slice(0, 3)\\n    df[\\\"shift-10\\\"] = df.groupby(\\\"n_frase\\\")[\\\"word\\\"].shift(1).str.slice(-10)\\n    df[\\\"shift+10\\\"] = df.groupby(\\\"n_frase\\\")[\\\"word\\\"].shift(-1).str.slice(0, 10)\\n\\n    df[\\\"shift-3\\\"].fillna(\\\"BOF\\\", inplace=True)\\n    df[\\\"shift+3\\\"].fillna(\\\"EOF\\\", inplace=True)\\n    df[\\\"shift-10\\\"].fillna(\\\"BOF\\\", inplace=True)\\n    df[\\\"shift+10\\\"].fillna(\\\"EOF\\\", inplace=True)\\n\\n    if spacy:\\n        df[\\\"spacy\\\"] = df[\\\"word\\\"].apply(spacy_entities_extractor)\\n\\n    df[\\\"bias\\\"] = 1\\n\\n    try:\\n        df.drop(columns=[\\\"tag\\\"], inplace=True)\\n    except:\\n        pass\\n\\n    return df\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extend_data(df: pd.DataFrame, spacy: bool = False) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "    Estende i dati attraverso un algortimo personalizzato\n",
    "    \"\"\"\n",
    "\n",
    "    df[\"shift-3\"] = df.groupby(\"n_frase\")[\"word\"].shift(1).str.slice(-3)\n",
    "    df[\"shift+3\"] = df.groupby(\"n_frase\")[\"word\"].shift(-1).str.slice(0, 3)\n",
    "    df[\"shift-10\"] = df.groupby(\"n_frase\")[\"word\"].shift(1).str.slice(-10)\n",
    "    df[\"shift+10\"] = df.groupby(\"n_frase\")[\"word\"].shift(-1).str.slice(0, 10)\n",
    "\n",
    "    df[\"shift-3\"].fillna(\"BOF\", inplace=True)\n",
    "    df[\"shift+3\"].fillna(\"EOF\", inplace=True)\n",
    "    df[\"shift-10\"].fillna(\"BOF\", inplace=True)\n",
    "    df[\"shift+10\"].fillna(\"EOF\", inplace=True)\n",
    "\n",
    "    if spacy:\n",
    "        df[\"spacy\"] = df[\"word\"].apply(spacy_entities_extractor)\n",
    "\n",
    "    df[\"bias\"] = 1\n",
    "\n",
    "    try:\n",
    "        df.drop(columns=[\"tag\"], inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96f10fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"regex_str = r\\\"\\\\[(?P<name>[a-zA-Z_]+)\\\\]\\\\((?P<value>[a-zA-Z\\\\' ]+)\\\\)+\\\"\\nslot_match = re.compile(regex_str)\";\n",
       "                var nbb_formatted_code = \"regex_str = r\\\"\\\\[(?P<name>[a-zA-Z_]+)\\\\]\\\\((?P<value>[a-zA-Z\\\\' ]+)\\\\)+\\\"\\nslot_match = re.compile(regex_str)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regex_str = r\"\\[(?P<name>[a-zA-Z_]+)\\]\\((?P<value>[a-zA-Z\\' ]+)\\)+\"\n",
    "slot_match = re.compile(regex_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c16aa708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"def conserva_solo_slot_name(text: str) -> str:\\n\\n    \\\"\\\"\\\"\\n    Per facilitare la riduzione dello spazio dimensionale\\n        vengono eliminati i valori degli slots\\n        mentre vengono conservati i loro nomi\\n\\n\\n    Parameters:\\n    -----------\\n\\n    text : str\\n\\n        \\u00e8 la stringa che contiene la frase\\n\\n\\n    Returns:\\n    -----------\\n    text : str\\n\\n    \\\"\\\"\\\"\\n\\n    text = rimuovi_punteggiatura(text)\\n\\n    pattern = r\\\"(\\\\([A-Za-z0-9 ]+\\\\)|\\\\[|\\\\])\\\"\\n\\n    text = re.sub(pattern, \\\"\\\", text)\\n\\n    return text\";\n",
       "                var nbb_formatted_code = \"def conserva_solo_slot_name(text: str) -> str:\\n\\n    \\\"\\\"\\\"\\n    Per facilitare la riduzione dello spazio dimensionale\\n        vengono eliminati i valori degli slots\\n        mentre vengono conservati i loro nomi\\n\\n\\n    Parameters:\\n    -----------\\n\\n    text : str\\n\\n        \\u00e8 la stringa che contiene la frase\\n\\n\\n    Returns:\\n    -----------\\n    text : str\\n\\n    \\\"\\\"\\\"\\n\\n    text = rimuovi_punteggiatura(text)\\n\\n    pattern = r\\\"(\\\\([A-Za-z0-9 ]+\\\\)|\\\\[|\\\\])\\\"\\n\\n    text = re.sub(pattern, \\\"\\\", text)\\n\\n    return text\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def conserva_solo_slot_name(text: str) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    Per facilitare la riduzione dello spazio dimensionale\n",
    "        vengono eliminati i valori degli slots\n",
    "        mentre vengono conservati i loro nomi\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "\n",
    "    text : str\n",
    "\n",
    "        è la stringa che contiene la frase\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    text : str\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    text = rimuovi_punteggiatura(text)\n",
    "\n",
    "    pattern = r\"(\\([A-Za-z0-9 ]+\\)|\\[|\\])\"\n",
    "\n",
    "    text = re.sub(pattern, \"\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ce420f",
   "metadata": {},
   "source": [
    "**Operazioni:**\n",
    "1. La classe deve verificare se esistono già i file pickle che conservano: modello, vocabolario e crf. Eventualmente parte il training comprensivo di generazione frase;\n",
    "2. Se i dati sono caricati allora può elaborare le frasi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d9e65c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"class BotAI:\\n    def __init__(self, db_file: str, model_path: str = \\\"models\\\"):\\n\\n        self.model_path = model_path\\n        self.db_file = db_file\\n\\n        self.classifier_pickle = \\\"classifier.pickle\\\"\\n        self.label_encoder_pickle = \\\"label_encoder.pickle\\\"\\n        self.crf_model_pickle = \\\"crf_model.pickle\\\"\\n        self.vocabulary_pickle = \\\"vocabulary.pickle\\\"\\n\\n        self.status = \\\"0 - starter\\\"\\n\\n        self.lookups = None\\n\\n        self.user_sentences = defaultdict(list)\\n        self.bot_sentences = defaultdict(list)\\n        self.dialogs = defaultdict(list)\\n\\n        self.sentences_file = \\\"sentences_origin.txt\\\"\\n        self.sentences_file_generated = \\\"sentences_generated.txt\\\"\\n        self.n_sentences = 1000\\n\\n        self.X = list()\\n        self.y = list()\\n\\n        self.cm = None\\n\\n        if self.check_model() == False:  # Verifica iniziale\\n            self.crf = sklearn_crfsuite.CRF(\\n                algorithm=\\\"lbfgs\\\",\\n                c1=0.1,\\n                c2=0.1,\\n                max_iterations=100,\\n                all_possible_states=False,\\n                all_possible_transitions=False,\\n            )\\n\\n            self.cv = CountVectorizer()\\n            self.le = LabelEncoder()\\n\\n            self.classifier = SGDClassifier(fit_intercept=False, loss=\\\"log\\\")\\n\\n            self.training()\\n        else:\\n            # Carica i modelli\\n            load_model = open(\\n                os.path.join(self.model_path, self.crf_model_pickle), \\\"rb\\\"\\n            )\\n            self.crf = pickle.load(load_model)\\n\\n            load_model = open(\\n                os.path.join(self.model_path, self.vocabulary_pickle), \\\"rb\\\"\\n            )\\n            self.cv = pickle.load(load_model)\\n\\n            load_model = open(\\n                os.path.join(self.model_path, self.label_encoder_pickle), \\\"rb\\\"\\n            )\\n            self.le = pickle.load(load_model)\\n\\n            load_model = open(\\n                os.path.join(self.model_path, self.classifier_pickle), \\\"rb\\\"\\n            )\\n            self.classifier = pickle.load(load_model)\\n\\n            self.build_lookup_tables()\\n\\n            self.sentences_extractor()\\n\\n            self.is_ready()\\n\\n            pass\\n\\n        return None\\n\\n    def check_model(self):\\n\\n        \\\"\\\"\\\"Cerca se ci sono gi\\u00e0 i modelli per la comprensione del testo\\\"\\\"\\\"\\n\\n        risp = False\\n\\n        if self.classifier_pickle in os.listdir(self.model_path):\\n            if self.label_encoder_pickle in os.listdir(self.model_path):\\n                if self.crf_model_pickle in os.listdir(self.model_path):\\n                    if self.vocabulary_pickle in os.listdir(self.model_path):\\n                        risp = True\\n        #         return [\\n        #             self.classifier_pickle,\\n        #             self.label_encoder_pickle,\\n        #             self.crf_model_pickle,\\n        #             self.vocabulary_pickle,\\n        #         ] == os.listdir(self.model_path)\\n\\n        return risp\\n\\n    def get_status(self) -> str:\\n\\n        \\\"\\\"\\\"Restituisce lo stato attuale\\\"\\\"\\\"\\n\\n        return self.status\\n\\n    def is_ready(self) -> bool:\\n\\n        \\\"\\\"\\\"Verifica se l'istanza \\u00e8 pronta\\\"\\\"\\\"\\n\\n        if self.check_model():\\n            self.status = \\\"ready\\\"\\n            return True\\n        else:\\n            return False\\n\\n    def build_lookup_tables(self) -> bool:\\n\\n        df_entities = pd.read_excel(\\\"dataset.xlsx\\\", sheet_name=\\\"entities_slots\\\")\\n\\n        self.lookups = remove_nan(df_entities)\\n\\n        self.status = \\\"1 - lookups tables loaded\\\"\\n\\n        return True\\n\\n    def sentences_extractor(self) -> bool:\\n\\n        \\\"\\\"\\\"Estrae le frasi dell'utente (le salva in un file txt), del bot, della conversazione\\\"\\\"\\\"\\n\\n        # UTENTE\\n\\n        df_user = pd.read_excel(\\\"dataset.xlsx\\\", sheet_name=\\\"user\\\", header=None)\\n        df_user.columns = [\\\"user\\\", \\\"sentences\\\"]\\n        df_user[\\\"user\\\"] = df_user[\\\"user\\\"].fillna(method=\\\"ffill\\\", axis=0)\\n\\n        df_grouped = df_user.groupby(\\\"user\\\")\\n\\n        for group in df_grouped.groups:\\n\\n            self.user_sentences[group] = df_grouped.get_group(group)[\\n                \\\"sentences\\\"\\n            ].tolist()\\n\\n        # FILE CON LE FRASI\\n\\n        i = True\\n\\n        for k, v in self.user_sentences.items():\\n\\n            for values in v:\\n\\n                if i:\\n                    with open(self.sentences_file, \\\"w+\\\") as f:\\n                        f.writelines(f\\\"{values}|{k}\\\\n\\\")\\n                        i = False\\n                else:\\n                    with open(self.sentences_file, \\\"a+\\\") as f:\\n                        f.writelines(f\\\"{values}|{k}\\\\n\\\")\\n\\n        # BOT\\n\\n        df_bot = pd.read_excel(\\\"dataset.xlsx\\\", sheet_name=\\\"bot\\\", header=None)\\n        df_bot.columns = [\\\"bot\\\", \\\"sentences\\\"]\\n        df_bot[\\\"bot\\\"] = df_bot[\\\"bot\\\"].fillna(method=\\\"ffill\\\", axis=0)\\n\\n        df_grouped = df_bot.groupby(\\\"bot\\\")\\n\\n        for group in df_grouped.groups:\\n\\n            self.bot_sentences[group] = df_grouped.get_group(group)[\\n                \\\"sentences\\\"\\n            ].tolist()\\n\\n        # DIALOGS\\n\\n        df_dialogs = pd.read_excel(\\\"dataset.xlsx\\\", sheet_name=\\\"dialogs\\\")\\n\\n        self.dialogs = remove_nan(df_dialogs)\\n\\n        self.dialogs = {v[0]: v[1] for v in list(self.dialogs.values())}\\n\\n        self.df_query = pd.read_csv(self.db_file)\\n\\n        self.status = \\\"2 - estrazione dati completata\\\"\\n\\n        return True\\n\\n    def sentences_generator(self) -> bool:\\n\\n        sentences, categories = [], []\\n\\n        with open(self.sentences_file, encoding=\\\"utf-8\\\") as f:\\n            dataset = f.read()\\n            dataset = dataset.split(\\\"\\\\n\\\")\\n\\n        for data in dataset:\\n            sentence = data.split(\\\"|\\\")\\n\\n            if len(sentence) > 1:\\n\\n                # TODO: Lasciare upper ?\\n\\n                sentences.append(sentence[0].upper())\\n                categories.append(sentence[1])\\n\\n        assert len(sentences) == len(categories)\\n\\n        slots = list(self.lookups.keys())\\n\\n        for i in range(self.n_sentences):\\n\\n            index = random.randint(0, len(sentences) - 1)\\n\\n            sentence = sentences[index]\\n            category = categories[index]\\n\\n            for key in slots:\\n\\n                \\\"\\\"\\\"\\n                Ogni volta che regex individua lo slot nella frase\\n                    sostituisce il valore con uno estratto in modo casuale\\n                \\\"\\\"\\\"\\n\\n                # TODO: Verificare\\n\\n                regex_str = fr\\\"\\\\[{key}\\\\]\\\\((?P<value>[a-z ]+)\\\\)+\\\"\\n\\n                slot_match = re.compile(regex_str)\\n\\n                repl = f\\\"[{key}]({random.choice(self.lookups[key])})\\\"\\n\\n                sentence = slot_match.sub(repl, sentence)\\n\\n                \\\"\\\"\\\"\\n                Poi riassocia la categoria di partenza\\n                \\\"\\\"\\\"\\n\\n            if i == 0:\\n\\n                with open(self.sentences_file_generated, \\\"w+\\\") as f:\\n                    f.writelines(f\\\"{sentence}|{category}\\\\n\\\")\\n\\n            else:\\n\\n                with open(self.sentences_file_generated, \\\"a+\\\") as f:\\n                    f.writelines(f\\\"{sentence}|{category}\\\\n\\\")\\n\\n        self.status = \\\"3 - frasi generate\\\"\\n\\n        return True\\n\\n    def prepare_data_crf(self) -> bool:\\n\\n        with open(self.sentences_file_generated, encoding=\\\"utf-8\\\") as f:\\n            sentences = f.read()\\n            sentences = sentences.split(\\\"\\\\n\\\")\\n\\n        arr_sentences = list()\\n        arr_categories = list()\\n\\n        for n, sentence in enumerate(sentences):\\n\\n            try:\\n\\n                sentence, categ = sentence.split(\\\"|\\\")\\n\\n                sentence = rimuovi_punteggiatura(sentence)\\n\\n                arr_categories.append(categ)\\n\\n                splits = slot_match.split(sentence)\\n\\n                #         match = slot_match.search(frase)\\n                matches = slot_match.findall(sentence)\\n\\n                dct = {k: v for k, v in matches}\\n\\n                if matches is not None:\\n\\n                    for split in splits:\\n                        if split in list(dct.values()):\\n                            for value in split.split():\\n\\n                                index = list(dct.values()).index(split)\\n                                key = list(dct.keys())[index]\\n\\n                                arr_sentences.append([n, value, key])\\n                        elif split in list(dct.keys()):\\n                            pass\\n                        else:\\n                            for value in split.split():\\n                                arr_sentences.append([n, value, \\\"O\\\"])\\n\\n                else:\\n                    \\\"\\\"\\\"\\n                    Serve per verificare se in qualche frase non avviene il match\\n                    \\\"\\\"\\\"\\n                    print(n, frase)\\n\\n            except Exception as err:\\n                pass\\n\\n        df = pd.DataFrame(arr_sentences, columns=[\\\"n_frase\\\", \\\"word\\\", \\\"tag\\\"])\\n        df_target = df[[\\\"n_frase\\\", \\\"tag\\\"]]\\n\\n        for k, v in df_target.groupby(\\\"n_frase\\\"):\\n\\n            self.y.append(v[\\\"tag\\\"].tolist())\\n\\n        df = extend_data(df, spacy=False)\\n\\n        for k, v in df.groupby(\\\"n_frase\\\"):\\n\\n            v.drop(columns=\\\"n_frase\\\", inplace=True)\\n\\n            self.X.append(v.to_dict(\\\"records\\\"))\\n\\n        self.status = \\\"4 - dati pronti per il training del crf\\\"\\n\\n        return True\\n\\n    #### INIZIO FUNZIONI PER IL CONDITIONAL RANDOM FIELD ####\\n\\n    def training_crf(self) -> bool:\\n\\n        self.crf.fit(self.X, self.y)\\n\\n        save_model = open(os.path.join(self.model_path, self.crf_model_pickle), \\\"wb\\\")\\n        pickle.dump(self.crf, save_model)\\n        save_model.close()\\n\\n        self.status = \\\"5 - crf model trained\\\"\\n\\n    def prepare_sentence(self, sentence: str) -> [pd.DataFrame, list]:\\n\\n        \\\"\\\"\\\"\\n        Prepare la frase per il predict\\n\\n        Parameters:\\n        -----------\\n\\n        sentence : str\\n\\n            \\u00e8 la frase che sar\\u00e0 elaborata\\n\\n\\n        Returns:\\n        -----------\\n        df : DataFrame\\n\\n        X_arr[0] : array\\n\\n            dati in formato utile a CRF per il predict\\n\\n        \\\"\\\"\\\"\\n\\n        sentence = rimuovi_punteggiatura(sentence)\\n\\n        X_arr = list()\\n\\n        df = pd.DataFrame(data=[i for i in sentence.split()], columns=[\\\"word\\\"])\\n        df[\\\"n_frase\\\"] = 1\\n\\n        df = extend_data(df)\\n\\n        for k, v in df.groupby(\\\"n_frase\\\"):\\n\\n            v.drop(columns=\\\"n_frase\\\", inplace=True)\\n\\n            X_arr.append(v.to_dict(\\\"records\\\"))\\n\\n        return df, X_arr[0]\\n\\n    def extend_sentence(self, sentence: str) -> pd.DataFrame:\\n\\n        \\\"\\\"\\\"\\n        Estrae slots e lo aggiunge al DataFrame come colonna\\n\\n        Parameters:\\n        -----------\\n\\n        sentence : str\\n\\n            \\u00e8 la stringa che contiene la frase\\n\\n        model : sklearn_crfsuite.estimator.CRF\\n\\n            \\u00e8 il modello addestrato di ConditionalRandomField\\n\\n\\n        Returns:\\n        -----------\\n        df : DataFrame\\n\\n            al DataFrame di partenza viene aggiunta una colonna\\n            con l'indicazione del tipo di slot individuato\\n\\n        \\\"\\\"\\\"\\n\\n        df, X_arr = self.prepare_sentence(sentence)\\n\\n        df[\\\"slots\\\"] = self.crf.predict_single(X_arr)\\n\\n        return df\\n\\n    #### DUCKLING ####\\n\\n    def extract_datetime(self, text: str, url=\\\"http://0.0.0.0:8000/parse\\\"):\\n\\n        data = {\\\"locale\\\": \\\"it_IT\\\", \\\"text\\\": text}\\n\\n        resp = None\\n\\n        datetimes = list()\\n\\n        try:\\n\\n            response = requests.post(url, data=data)\\n\\n            try:\\n\\n                if response.status_code == 200:\\n\\n                    for dt in response.json():\\n\\n                        if dt[\\\"dim\\\"] == \\\"time\\\":\\n\\n                            dtime = dt[\\\"value\\\"][\\\"value\\\"]\\n                            dtime = dateparser.parse(dtime)\\n\\n                            datetimes.append(dtime)\\n\\n                resp = dict()\\n\\n                if len(datetimes) > 1:\\n                    resp[\\\"datetime\\\"] = list([min(datetimes), max(datetimes)])\\n                else:\\n                    resp[\\\"datetime\\\"] = list(datetimes)\\n\\n            except:\\n                pass\\n\\n        except:\\n            pass\\n\\n        return resp\\n\\n    def slots_extractor(self, sentence: str) -> defaultdict:\\n\\n        \\\"\\\"\\\"\\n        Restituisce un dictionary degli slots individuati\\n\\n        Parameters:\\n        -----------\\n\\n        sentence : str\\n\\n            \\u00e8 la stringa che contiene la frase\\n\\n        model : sklearn_crfsuite.estimator.CRF\\n\\n            \\u00e8 il modello addestrato di ConditionalRandomField\\n\\n\\n        Returns:\\n        -----------\\n        dd : dictionary\\n\\n        \\\"\\\"\\\"\\n\\n        df = self.extend_sentence(sentence)\\n\\n        dd = defaultdict(list)\\n\\n        for k, v in df.query(\\\"slots != 'O'\\\").groupby(\\\"slots\\\"):\\n            dd[k] = \\\" \\\".join(v[\\\"word\\\"])\\n\\n        \\\"\\\"\\\"\\n        Estrazione date ed orari tramite duckling    \\n        \\\"\\\"\\\"\\n\\n        date_time = self.extract_datetime(sentence)\\n\\n        if date_time is not None and len(date_time) > 0:\\n            dd[\\\"DATETIMES\\\"] = date_time[\\\"datetime\\\"]\\n\\n        return dd\\n\\n    #### FINE FUNZIONI PER IL CONDITIONAL RANDOM FIELD ####\\n\\n    #### INIZIO FUNZIONI PER LA CLASSIFICAZIONE DELL'INTENT ####\\n\\n    def replace_slot_values(self, sentence_dict: dict) -> dict:\\n\\n        \\\"\\\"\\\"\\n        Integrazione dictionary - parte 1 di 2\\n\\n\\n        Per operare una riduzione delle variabili\\n        sostituisce il valore con il relativo slot\\n        \\\"\\\"\\\"\\n\\n        sentence_dict[\\\"replaced_sentence\\\"] = sentence_dict[\\\"sentence\\\"]\\n\\n        for k, v in sentence_dict[\\\"slots\\\"].items():\\n\\n            if k != \\\"DATETIMES\\\":\\n\\n                sentence_dict[\\\"replaced_sentence\\\"] = re.sub(\\n                    v, k, sentence_dict[\\\"replaced_sentence\\\"]\\n                )\\n\\n        return sentence_dict\\n\\n    def add_slots(self, sentence: str) -> dict:\\n\\n        \\\"\\\"\\\"\\n        Integrazione dictionary - parte 2 di 2\\n        \\\"\\\"\\\"\\n\\n        sentence_dict = {}\\n\\n        sentence = rimuovi_punteggiatura(sentence)\\n\\n        slots_dict = self.slots_extractor(sentence)\\n\\n        sentence_dict[\\\"sentence\\\"] = sentence\\n        sentence_dict[\\\"slots\\\"] = slots_dict\\n\\n        sentence_dict = self.replace_slot_values(sentence_dict)\\n\\n        #     date_time = extract_datetime(sentence)\\n\\n        #     if date_time is not None and len(date_time) > 0:\\n        #         sentence_dict['datetimes'] = date_time['datetime']\\n        #     else:\\n        #         sentence_dict['datetimes'] = False\\n\\n        return sentence_dict\\n\\n    def training_intents(self) -> bool:\\n\\n        \\\"\\\"\\\"Addestramento algoritmo di classificazione intents\\\"\\\"\\\"\\n\\n        df = pd.read_csv(self.sentences_file_generated, sep=\\\"|\\\", header=None)\\n\\n        df.columns = [\\\"sentences\\\", \\\"intents\\\"]\\n\\n        df[\\\"sentences\\\"] = df[\\\"sentences\\\"].apply(conserva_solo_slot_name)\\n\\n        self.cv.fit(df.sentences)\\n\\n        with open(\\n            os.path.join(self.model_path, self.vocabulary_pickle), \\\"wb\\\"\\n        ) as vocabs:\\n            pickle.dump(self.cv, vocabs)\\n\\n        #### GENERAZIONE FRASI FAKE ####\\n\\n        fake_list = list()\\n\\n        for i in range(30):\\n            fake_list.append(\\n                \\\" \\\".join(random.choices(list(self.cv.vocabulary_.keys()), k=10))\\n            )\\n\\n        df_fake = pd.DataFrame({\\\"sentences\\\": fake_list, \\\"intents\\\": \\\"fake\\\"})\\n\\n        df = pd.concat([df, df_fake], axis=0, ignore_index=True)\\n\\n        #### LABEL ENCODER ####\\n\\n        self.le.fit(df.intents.unique())\\n\\n        with open(\\n            os.path.join(self.model_path, self.label_encoder_pickle), \\\"wb\\\"\\n        ) as label_encoder:\\n            pickle.dump(self.le, label_encoder)\\n\\n        labels_categories = self.le.transform(df.intents.values)\\n\\n        self.classifier.fit(\\n            X=self.cv.transform(df[\\\"sentences\\\"].values).toarray(), y=labels_categories\\n        )\\n\\n        with open(os.path.join(self.model_path, self.classifier_pickle), \\\"wb\\\") as cls:\\n            pickle.dump(self.classifier, cls)\\n\\n        predicted_categories = [\\n            self.classifier.predict(\\n                [np.max(self.cv.transform(sentence.split()).toarray(), axis=0)]\\n            )[0]\\n            for sentence in df.sentences.values\\n        ]\\n\\n        self.cm = confusion_matrix(labels_categories, predicted_categories)\\n\\n        self.status = \\\"6 - classifier model trained\\\"\\n\\n        return True\\n\\n    #### FINE FUNZIONI PER LA CLASSIFICAZIONE DELL'INTENT ####\\n\\n    def get_intents_and_slots(self, sentence: str, threasold=0.25) -> dict:\\n\\n        \\\"\\\"\\\"\\n        Estrae gli intents e gli slots dalla frase\\n        \\\"\\\"\\\"\\n\\n        sentence_dict = self.add_slots(sentence)\\n\\n        sentence = sentence_dict[\\\"replaced_sentence\\\"]\\n\\n        arr = self.cv.transform(\\n            self.add_slots(sentence)[\\\"replaced_sentence\\\"].split()\\n        ).toarray()\\n\\n        arr = np.max(arr, axis=0)\\n\\n        probs = self.classifier.predict_proba([arr])[0]\\n\\n        df = pd.DataFrame({\\\"classes\\\": self.le.classes_, \\\"probs\\\": probs})\\n        df.sort_values(\\\"probs\\\", ascending=False, inplace=True)\\n        classes_with_prob = df[df[\\\"probs\\\"] > threasold].to_dict(\\\"records\\\")\\n        sentence_dict[\\\"intents\\\"] = classes_with_prob\\n\\n        max_intent = self.le.classes_[np.argmax(probs)]\\n        sentence_dict[\\\"max_intent\\\"] = max_intent\\n\\n        return sentence_dict\\n\\n    def bot_reply(self, sentence: str, threasold=0.25) -> dict:\\n\\n        \\\"\\\"\\\"\\n        Genera la risposta\\n        \\\"\\\"\\\"\\n\\n        sentence = sentence.upper()\\n\\n        data = self.get_intents_and_slots(sentence, threasold)\\n\\n        if (data[\\\"intents\\\"][0][\\\"probs\\\"] > 0.75) and data[\\\"intents\\\"][0][\\n            \\\"classes\\\"\\n        ] != \\\"fake\\\":\\n\\n            intent = data[\\\"max_intent\\\"]\\n\\n            reply = self.dialogs[intent]\\n\\n            reply_str = random.choice(self.bot_sentences[reply])\\n\\n            query_list = list()\\n\\n            for k, v in data[\\\"slots\\\"].items():\\n                query_list.append(f\\\"{k} == '{v}'\\\")\\n\\n                k = list(data[\\\"slots\\\"].keys())[0]\\n                k = f\\\"[{k}]\\\"\\n\\n                print(k)\\n\\n                v = list(data[\\\"slots\\\"].values())[0]\\n\\n                reply_str = re.sub(fr\\\"[{k}+]\\\", v, reply_str)\\n\\n            query = \\\" and \\\".join(query_list)\\n\\n            n = self.df_query.query(query)[\\\"NOME\\\"].drop_duplicates().count()\\n\\n            return reply_str % (n)\\n\\n        else:\\n\\n            intent = \\\"Non ho capito, riformula meglio la tua domanda\\\"\\n\\n            return intent\\n\\n    def training(self):\\n\\n        self.build_lookup_tables()\\n\\n        self.sentences_extractor()\\n\\n        self.sentences_generator()\\n\\n        self.prepare_data_crf()\\n\\n        self.training_crf()\\n\\n        self.training_intents()\\n\\n        return True\";\n",
       "                var nbb_formatted_code = \"class BotAI:\\n    def __init__(self, db_file: str, model_path: str = \\\"models\\\"):\\n\\n        self.model_path = model_path\\n        self.db_file = db_file\\n\\n        self.classifier_pickle = \\\"classifier.pickle\\\"\\n        self.label_encoder_pickle = \\\"label_encoder.pickle\\\"\\n        self.crf_model_pickle = \\\"crf_model.pickle\\\"\\n        self.vocabulary_pickle = \\\"vocabulary.pickle\\\"\\n\\n        self.status = \\\"0 - starter\\\"\\n\\n        self.lookups = None\\n\\n        self.user_sentences = defaultdict(list)\\n        self.bot_sentences = defaultdict(list)\\n        self.dialogs = defaultdict(list)\\n\\n        self.sentences_file = \\\"sentences_origin.txt\\\"\\n        self.sentences_file_generated = \\\"sentences_generated.txt\\\"\\n        self.n_sentences = 1000\\n\\n        self.X = list()\\n        self.y = list()\\n\\n        self.cm = None\\n\\n        if self.check_model() == False:  # Verifica iniziale\\n            self.crf = sklearn_crfsuite.CRF(\\n                algorithm=\\\"lbfgs\\\",\\n                c1=0.1,\\n                c2=0.1,\\n                max_iterations=100,\\n                all_possible_states=False,\\n                all_possible_transitions=False,\\n            )\\n\\n            self.cv = CountVectorizer()\\n            self.le = LabelEncoder()\\n\\n            self.classifier = SGDClassifier(fit_intercept=False, loss=\\\"log\\\")\\n\\n            self.training()\\n        else:\\n            # Carica i modelli\\n            load_model = open(\\n                os.path.join(self.model_path, self.crf_model_pickle), \\\"rb\\\"\\n            )\\n            self.crf = pickle.load(load_model)\\n\\n            load_model = open(\\n                os.path.join(self.model_path, self.vocabulary_pickle), \\\"rb\\\"\\n            )\\n            self.cv = pickle.load(load_model)\\n\\n            load_model = open(\\n                os.path.join(self.model_path, self.label_encoder_pickle), \\\"rb\\\"\\n            )\\n            self.le = pickle.load(load_model)\\n\\n            load_model = open(\\n                os.path.join(self.model_path, self.classifier_pickle), \\\"rb\\\"\\n            )\\n            self.classifier = pickle.load(load_model)\\n\\n            self.build_lookup_tables()\\n\\n            self.sentences_extractor()\\n\\n            self.is_ready()\\n\\n            pass\\n\\n        return None\\n\\n    def check_model(self):\\n\\n        \\\"\\\"\\\"Cerca se ci sono gi\\u00e0 i modelli per la comprensione del testo\\\"\\\"\\\"\\n\\n        risp = False\\n\\n        if self.classifier_pickle in os.listdir(self.model_path):\\n            if self.label_encoder_pickle in os.listdir(self.model_path):\\n                if self.crf_model_pickle in os.listdir(self.model_path):\\n                    if self.vocabulary_pickle in os.listdir(self.model_path):\\n                        risp = True\\n        #         return [\\n        #             self.classifier_pickle,\\n        #             self.label_encoder_pickle,\\n        #             self.crf_model_pickle,\\n        #             self.vocabulary_pickle,\\n        #         ] == os.listdir(self.model_path)\\n\\n        return risp\\n\\n    def get_status(self) -> str:\\n\\n        \\\"\\\"\\\"Restituisce lo stato attuale\\\"\\\"\\\"\\n\\n        return self.status\\n\\n    def is_ready(self) -> bool:\\n\\n        \\\"\\\"\\\"Verifica se l'istanza \\u00e8 pronta\\\"\\\"\\\"\\n\\n        if self.check_model():\\n            self.status = \\\"ready\\\"\\n            return True\\n        else:\\n            return False\\n\\n    def build_lookup_tables(self) -> bool:\\n\\n        df_entities = pd.read_excel(\\\"dataset.xlsx\\\", sheet_name=\\\"entities_slots\\\")\\n\\n        self.lookups = remove_nan(df_entities)\\n\\n        self.status = \\\"1 - lookups tables loaded\\\"\\n\\n        return True\\n\\n    def sentences_extractor(self) -> bool:\\n\\n        \\\"\\\"\\\"Estrae le frasi dell'utente (le salva in un file txt), del bot, della conversazione\\\"\\\"\\\"\\n\\n        # UTENTE\\n\\n        df_user = pd.read_excel(\\\"dataset.xlsx\\\", sheet_name=\\\"user\\\", header=None)\\n        df_user.columns = [\\\"user\\\", \\\"sentences\\\"]\\n        df_user[\\\"user\\\"] = df_user[\\\"user\\\"].fillna(method=\\\"ffill\\\", axis=0)\\n\\n        df_grouped = df_user.groupby(\\\"user\\\")\\n\\n        for group in df_grouped.groups:\\n\\n            self.user_sentences[group] = df_grouped.get_group(group)[\\n                \\\"sentences\\\"\\n            ].tolist()\\n\\n        # FILE CON LE FRASI\\n\\n        i = True\\n\\n        for k, v in self.user_sentences.items():\\n\\n            for values in v:\\n\\n                if i:\\n                    with open(self.sentences_file, \\\"w+\\\") as f:\\n                        f.writelines(f\\\"{values}|{k}\\\\n\\\")\\n                        i = False\\n                else:\\n                    with open(self.sentences_file, \\\"a+\\\") as f:\\n                        f.writelines(f\\\"{values}|{k}\\\\n\\\")\\n\\n        # BOT\\n\\n        df_bot = pd.read_excel(\\\"dataset.xlsx\\\", sheet_name=\\\"bot\\\", header=None)\\n        df_bot.columns = [\\\"bot\\\", \\\"sentences\\\"]\\n        df_bot[\\\"bot\\\"] = df_bot[\\\"bot\\\"].fillna(method=\\\"ffill\\\", axis=0)\\n\\n        df_grouped = df_bot.groupby(\\\"bot\\\")\\n\\n        for group in df_grouped.groups:\\n\\n            self.bot_sentences[group] = df_grouped.get_group(group)[\\n                \\\"sentences\\\"\\n            ].tolist()\\n\\n        # DIALOGS\\n\\n        df_dialogs = pd.read_excel(\\\"dataset.xlsx\\\", sheet_name=\\\"dialogs\\\")\\n\\n        self.dialogs = remove_nan(df_dialogs)\\n\\n        self.dialogs = {v[0]: v[1] for v in list(self.dialogs.values())}\\n\\n        self.df_query = pd.read_csv(self.db_file)\\n\\n        self.status = \\\"2 - estrazione dati completata\\\"\\n\\n        return True\\n\\n    def sentences_generator(self) -> bool:\\n\\n        sentences, categories = [], []\\n\\n        with open(self.sentences_file, encoding=\\\"utf-8\\\") as f:\\n            dataset = f.read()\\n            dataset = dataset.split(\\\"\\\\n\\\")\\n\\n        for data in dataset:\\n            sentence = data.split(\\\"|\\\")\\n\\n            if len(sentence) > 1:\\n\\n                # TODO: Lasciare upper ?\\n\\n                sentences.append(sentence[0].upper())\\n                categories.append(sentence[1])\\n\\n        assert len(sentences) == len(categories)\\n\\n        slots = list(self.lookups.keys())\\n\\n        for i in range(self.n_sentences):\\n\\n            index = random.randint(0, len(sentences) - 1)\\n\\n            sentence = sentences[index]\\n            category = categories[index]\\n\\n            for key in slots:\\n\\n                \\\"\\\"\\\"\\n                Ogni volta che regex individua lo slot nella frase\\n                    sostituisce il valore con uno estratto in modo casuale\\n                \\\"\\\"\\\"\\n\\n                # TODO: Verificare\\n\\n                regex_str = fr\\\"\\\\[{key}\\\\]\\\\((?P<value>[a-z ]+)\\\\)+\\\"\\n\\n                slot_match = re.compile(regex_str)\\n\\n                repl = f\\\"[{key}]({random.choice(self.lookups[key])})\\\"\\n\\n                sentence = slot_match.sub(repl, sentence)\\n\\n                \\\"\\\"\\\"\\n                Poi riassocia la categoria di partenza\\n                \\\"\\\"\\\"\\n\\n            if i == 0:\\n\\n                with open(self.sentences_file_generated, \\\"w+\\\") as f:\\n                    f.writelines(f\\\"{sentence}|{category}\\\\n\\\")\\n\\n            else:\\n\\n                with open(self.sentences_file_generated, \\\"a+\\\") as f:\\n                    f.writelines(f\\\"{sentence}|{category}\\\\n\\\")\\n\\n        self.status = \\\"3 - frasi generate\\\"\\n\\n        return True\\n\\n    def prepare_data_crf(self) -> bool:\\n\\n        with open(self.sentences_file_generated, encoding=\\\"utf-8\\\") as f:\\n            sentences = f.read()\\n            sentences = sentences.split(\\\"\\\\n\\\")\\n\\n        arr_sentences = list()\\n        arr_categories = list()\\n\\n        for n, sentence in enumerate(sentences):\\n\\n            try:\\n\\n                sentence, categ = sentence.split(\\\"|\\\")\\n\\n                sentence = rimuovi_punteggiatura(sentence)\\n\\n                arr_categories.append(categ)\\n\\n                splits = slot_match.split(sentence)\\n\\n                #         match = slot_match.search(frase)\\n                matches = slot_match.findall(sentence)\\n\\n                dct = {k: v for k, v in matches}\\n\\n                if matches is not None:\\n\\n                    for split in splits:\\n                        if split in list(dct.values()):\\n                            for value in split.split():\\n\\n                                index = list(dct.values()).index(split)\\n                                key = list(dct.keys())[index]\\n\\n                                arr_sentences.append([n, value, key])\\n                        elif split in list(dct.keys()):\\n                            pass\\n                        else:\\n                            for value in split.split():\\n                                arr_sentences.append([n, value, \\\"O\\\"])\\n\\n                else:\\n                    \\\"\\\"\\\"\\n                    Serve per verificare se in qualche frase non avviene il match\\n                    \\\"\\\"\\\"\\n                    print(n, frase)\\n\\n            except Exception as err:\\n                pass\\n\\n        df = pd.DataFrame(arr_sentences, columns=[\\\"n_frase\\\", \\\"word\\\", \\\"tag\\\"])\\n        df_target = df[[\\\"n_frase\\\", \\\"tag\\\"]]\\n\\n        for k, v in df_target.groupby(\\\"n_frase\\\"):\\n\\n            self.y.append(v[\\\"tag\\\"].tolist())\\n\\n        df = extend_data(df, spacy=False)\\n\\n        for k, v in df.groupby(\\\"n_frase\\\"):\\n\\n            v.drop(columns=\\\"n_frase\\\", inplace=True)\\n\\n            self.X.append(v.to_dict(\\\"records\\\"))\\n\\n        self.status = \\\"4 - dati pronti per il training del crf\\\"\\n\\n        return True\\n\\n    #### INIZIO FUNZIONI PER IL CONDITIONAL RANDOM FIELD ####\\n\\n    def training_crf(self) -> bool:\\n\\n        self.crf.fit(self.X, self.y)\\n\\n        save_model = open(os.path.join(self.model_path, self.crf_model_pickle), \\\"wb\\\")\\n        pickle.dump(self.crf, save_model)\\n        save_model.close()\\n\\n        self.status = \\\"5 - crf model trained\\\"\\n\\n    def prepare_sentence(self, sentence: str) -> [pd.DataFrame, list]:\\n\\n        \\\"\\\"\\\"\\n        Prepare la frase per il predict\\n\\n        Parameters:\\n        -----------\\n\\n        sentence : str\\n\\n            \\u00e8 la frase che sar\\u00e0 elaborata\\n\\n\\n        Returns:\\n        -----------\\n        df : DataFrame\\n\\n        X_arr[0] : array\\n\\n            dati in formato utile a CRF per il predict\\n\\n        \\\"\\\"\\\"\\n\\n        sentence = rimuovi_punteggiatura(sentence)\\n\\n        X_arr = list()\\n\\n        df = pd.DataFrame(data=[i for i in sentence.split()], columns=[\\\"word\\\"])\\n        df[\\\"n_frase\\\"] = 1\\n\\n        df = extend_data(df)\\n\\n        for k, v in df.groupby(\\\"n_frase\\\"):\\n\\n            v.drop(columns=\\\"n_frase\\\", inplace=True)\\n\\n            X_arr.append(v.to_dict(\\\"records\\\"))\\n\\n        return df, X_arr[0]\\n\\n    def extend_sentence(self, sentence: str) -> pd.DataFrame:\\n\\n        \\\"\\\"\\\"\\n        Estrae slots e lo aggiunge al DataFrame come colonna\\n\\n        Parameters:\\n        -----------\\n\\n        sentence : str\\n\\n            \\u00e8 la stringa che contiene la frase\\n\\n        model : sklearn_crfsuite.estimator.CRF\\n\\n            \\u00e8 il modello addestrato di ConditionalRandomField\\n\\n\\n        Returns:\\n        -----------\\n        df : DataFrame\\n\\n            al DataFrame di partenza viene aggiunta una colonna\\n            con l'indicazione del tipo di slot individuato\\n\\n        \\\"\\\"\\\"\\n\\n        df, X_arr = self.prepare_sentence(sentence)\\n\\n        df[\\\"slots\\\"] = self.crf.predict_single(X_arr)\\n\\n        return df\\n\\n    #### DUCKLING ####\\n\\n    def extract_datetime(self, text: str, url=\\\"http://0.0.0.0:8000/parse\\\"):\\n\\n        data = {\\\"locale\\\": \\\"it_IT\\\", \\\"text\\\": text}\\n\\n        resp = None\\n\\n        datetimes = list()\\n\\n        try:\\n\\n            response = requests.post(url, data=data)\\n\\n            try:\\n\\n                if response.status_code == 200:\\n\\n                    for dt in response.json():\\n\\n                        if dt[\\\"dim\\\"] == \\\"time\\\":\\n\\n                            dtime = dt[\\\"value\\\"][\\\"value\\\"]\\n                            dtime = dateparser.parse(dtime)\\n\\n                            datetimes.append(dtime)\\n\\n                resp = dict()\\n\\n                if len(datetimes) > 1:\\n                    resp[\\\"datetime\\\"] = list([min(datetimes), max(datetimes)])\\n                else:\\n                    resp[\\\"datetime\\\"] = list(datetimes)\\n\\n            except:\\n                pass\\n\\n        except:\\n            pass\\n\\n        return resp\\n\\n    def slots_extractor(self, sentence: str) -> defaultdict:\\n\\n        \\\"\\\"\\\"\\n        Restituisce un dictionary degli slots individuati\\n\\n        Parameters:\\n        -----------\\n\\n        sentence : str\\n\\n            \\u00e8 la stringa che contiene la frase\\n\\n        model : sklearn_crfsuite.estimator.CRF\\n\\n            \\u00e8 il modello addestrato di ConditionalRandomField\\n\\n\\n        Returns:\\n        -----------\\n        dd : dictionary\\n\\n        \\\"\\\"\\\"\\n\\n        df = self.extend_sentence(sentence)\\n\\n        dd = defaultdict(list)\\n\\n        for k, v in df.query(\\\"slots != 'O'\\\").groupby(\\\"slots\\\"):\\n            dd[k] = \\\" \\\".join(v[\\\"word\\\"])\\n\\n        \\\"\\\"\\\"\\n        Estrazione date ed orari tramite duckling    \\n        \\\"\\\"\\\"\\n\\n        date_time = self.extract_datetime(sentence)\\n\\n        if date_time is not None and len(date_time) > 0:\\n            dd[\\\"DATETIMES\\\"] = date_time[\\\"datetime\\\"]\\n\\n        return dd\\n\\n    #### FINE FUNZIONI PER IL CONDITIONAL RANDOM FIELD ####\\n\\n    #### INIZIO FUNZIONI PER LA CLASSIFICAZIONE DELL'INTENT ####\\n\\n    def replace_slot_values(self, sentence_dict: dict) -> dict:\\n\\n        \\\"\\\"\\\"\\n        Integrazione dictionary - parte 1 di 2\\n\\n\\n        Per operare una riduzione delle variabili\\n        sostituisce il valore con il relativo slot\\n        \\\"\\\"\\\"\\n\\n        sentence_dict[\\\"replaced_sentence\\\"] = sentence_dict[\\\"sentence\\\"]\\n\\n        for k, v in sentence_dict[\\\"slots\\\"].items():\\n\\n            if k != \\\"DATETIMES\\\":\\n\\n                sentence_dict[\\\"replaced_sentence\\\"] = re.sub(\\n                    v, k, sentence_dict[\\\"replaced_sentence\\\"]\\n                )\\n\\n        return sentence_dict\\n\\n    def add_slots(self, sentence: str) -> dict:\\n\\n        \\\"\\\"\\\"\\n        Integrazione dictionary - parte 2 di 2\\n        \\\"\\\"\\\"\\n\\n        sentence_dict = {}\\n\\n        sentence = rimuovi_punteggiatura(sentence)\\n\\n        slots_dict = self.slots_extractor(sentence)\\n\\n        sentence_dict[\\\"sentence\\\"] = sentence\\n        sentence_dict[\\\"slots\\\"] = slots_dict\\n\\n        sentence_dict = self.replace_slot_values(sentence_dict)\\n\\n        #     date_time = extract_datetime(sentence)\\n\\n        #     if date_time is not None and len(date_time) > 0:\\n        #         sentence_dict['datetimes'] = date_time['datetime']\\n        #     else:\\n        #         sentence_dict['datetimes'] = False\\n\\n        return sentence_dict\\n\\n    def training_intents(self) -> bool:\\n\\n        \\\"\\\"\\\"Addestramento algoritmo di classificazione intents\\\"\\\"\\\"\\n\\n        df = pd.read_csv(self.sentences_file_generated, sep=\\\"|\\\", header=None)\\n\\n        df.columns = [\\\"sentences\\\", \\\"intents\\\"]\\n\\n        df[\\\"sentences\\\"] = df[\\\"sentences\\\"].apply(conserva_solo_slot_name)\\n\\n        self.cv.fit(df.sentences)\\n\\n        with open(\\n            os.path.join(self.model_path, self.vocabulary_pickle), \\\"wb\\\"\\n        ) as vocabs:\\n            pickle.dump(self.cv, vocabs)\\n\\n        #### GENERAZIONE FRASI FAKE ####\\n\\n        fake_list = list()\\n\\n        for i in range(30):\\n            fake_list.append(\\n                \\\" \\\".join(random.choices(list(self.cv.vocabulary_.keys()), k=10))\\n            )\\n\\n        df_fake = pd.DataFrame({\\\"sentences\\\": fake_list, \\\"intents\\\": \\\"fake\\\"})\\n\\n        df = pd.concat([df, df_fake], axis=0, ignore_index=True)\\n\\n        #### LABEL ENCODER ####\\n\\n        self.le.fit(df.intents.unique())\\n\\n        with open(\\n            os.path.join(self.model_path, self.label_encoder_pickle), \\\"wb\\\"\\n        ) as label_encoder:\\n            pickle.dump(self.le, label_encoder)\\n\\n        labels_categories = self.le.transform(df.intents.values)\\n\\n        self.classifier.fit(\\n            X=self.cv.transform(df[\\\"sentences\\\"].values).toarray(), y=labels_categories\\n        )\\n\\n        with open(os.path.join(self.model_path, self.classifier_pickle), \\\"wb\\\") as cls:\\n            pickle.dump(self.classifier, cls)\\n\\n        predicted_categories = [\\n            self.classifier.predict(\\n                [np.max(self.cv.transform(sentence.split()).toarray(), axis=0)]\\n            )[0]\\n            for sentence in df.sentences.values\\n        ]\\n\\n        self.cm = confusion_matrix(labels_categories, predicted_categories)\\n\\n        self.status = \\\"6 - classifier model trained\\\"\\n\\n        return True\\n\\n    #### FINE FUNZIONI PER LA CLASSIFICAZIONE DELL'INTENT ####\\n\\n    def get_intents_and_slots(self, sentence: str, threasold=0.25) -> dict:\\n\\n        \\\"\\\"\\\"\\n        Estrae gli intents e gli slots dalla frase\\n        \\\"\\\"\\\"\\n\\n        sentence_dict = self.add_slots(sentence)\\n\\n        sentence = sentence_dict[\\\"replaced_sentence\\\"]\\n\\n        arr = self.cv.transform(\\n            self.add_slots(sentence)[\\\"replaced_sentence\\\"].split()\\n        ).toarray()\\n\\n        arr = np.max(arr, axis=0)\\n\\n        probs = self.classifier.predict_proba([arr])[0]\\n\\n        df = pd.DataFrame({\\\"classes\\\": self.le.classes_, \\\"probs\\\": probs})\\n        df.sort_values(\\\"probs\\\", ascending=False, inplace=True)\\n        classes_with_prob = df[df[\\\"probs\\\"] > threasold].to_dict(\\\"records\\\")\\n        sentence_dict[\\\"intents\\\"] = classes_with_prob\\n\\n        max_intent = self.le.classes_[np.argmax(probs)]\\n        sentence_dict[\\\"max_intent\\\"] = max_intent\\n\\n        return sentence_dict\\n\\n    def bot_reply(self, sentence: str, threasold=0.25) -> dict:\\n\\n        \\\"\\\"\\\"\\n        Genera la risposta\\n        \\\"\\\"\\\"\\n\\n        sentence = sentence.upper()\\n\\n        data = self.get_intents_and_slots(sentence, threasold)\\n\\n        if (data[\\\"intents\\\"][0][\\\"probs\\\"] > 0.75) and data[\\\"intents\\\"][0][\\n            \\\"classes\\\"\\n        ] != \\\"fake\\\":\\n\\n            intent = data[\\\"max_intent\\\"]\\n\\n            reply = self.dialogs[intent]\\n\\n            reply_str = random.choice(self.bot_sentences[reply])\\n\\n            query_list = list()\\n\\n            for k, v in data[\\\"slots\\\"].items():\\n                query_list.append(f\\\"{k} == '{v}'\\\")\\n\\n                k = list(data[\\\"slots\\\"].keys())[0]\\n                k = f\\\"[{k}]\\\"\\n\\n                print(k)\\n\\n                v = list(data[\\\"slots\\\"].values())[0]\\n\\n                reply_str = re.sub(fr\\\"[{k}+]\\\", v, reply_str)\\n\\n            query = \\\" and \\\".join(query_list)\\n\\n            n = self.df_query.query(query)[\\\"NOME\\\"].drop_duplicates().count()\\n\\n            return reply_str % (n)\\n\\n        else:\\n\\n            intent = \\\"Non ho capito, riformula meglio la tua domanda\\\"\\n\\n            return intent\\n\\n    def training(self):\\n\\n        self.build_lookup_tables()\\n\\n        self.sentences_extractor()\\n\\n        self.sentences_generator()\\n\\n        self.prepare_data_crf()\\n\\n        self.training_crf()\\n\\n        self.training_intents()\\n\\n        return True\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class BotAI:\n",
    "    def __init__(self, db_file: str, model_path: str = \"models\"):\n",
    "\n",
    "        self.model_path = model_path\n",
    "        self.db_file = db_file\n",
    "\n",
    "        self.classifier_pickle = \"classifier.pickle\"\n",
    "        self.label_encoder_pickle = \"label_encoder.pickle\"\n",
    "        self.crf_model_pickle = \"crf_model.pickle\"\n",
    "        self.vocabulary_pickle = \"vocabulary.pickle\"\n",
    "\n",
    "        self.status = \"0 - starter\"\n",
    "\n",
    "        self.lookups = None\n",
    "\n",
    "        self.user_sentences = defaultdict(list)\n",
    "        self.bot_sentences = defaultdict(list)\n",
    "        self.dialogs = defaultdict(list)\n",
    "\n",
    "        self.sentences_file = \"sentences_origin.txt\"\n",
    "        self.sentences_file_generated = \"sentences_generated.txt\"\n",
    "        self.n_sentences = 1000\n",
    "\n",
    "        self.X = list()\n",
    "        self.y = list()\n",
    "\n",
    "        self.cm = None\n",
    "\n",
    "        if self.check_model() == False:  # Verifica iniziale\n",
    "            self.crf = sklearn_crfsuite.CRF(\n",
    "                algorithm=\"lbfgs\",\n",
    "                c1=0.1,\n",
    "                c2=0.1,\n",
    "                max_iterations=100,\n",
    "                all_possible_states=False,\n",
    "                all_possible_transitions=False,\n",
    "            )\n",
    "\n",
    "            self.cv = CountVectorizer()\n",
    "            self.le = LabelEncoder()\n",
    "\n",
    "            self.classifier = SGDClassifier(fit_intercept=False, loss=\"log\")\n",
    "\n",
    "            self.training()\n",
    "        else:\n",
    "            # Carica i modelli\n",
    "            \n",
    "            # TODO: Make it DRY\n",
    "            \n",
    "            load_model = open(\n",
    "                os.path.join(self.model_path, self.crf_model_pickle), \"rb\"\n",
    "            )\n",
    "            self.crf = pickle.load(load_model)\n",
    "\n",
    "            load_model = open(\n",
    "                os.path.join(self.model_path, self.vocabulary_pickle), \"rb\"\n",
    "            )\n",
    "            self.cv = pickle.load(load_model)\n",
    "\n",
    "            load_model = open(\n",
    "                os.path.join(self.model_path, self.label_encoder_pickle), \"rb\"\n",
    "            )\n",
    "            self.le = pickle.load(load_model)\n",
    "\n",
    "            load_model = open(\n",
    "                os.path.join(self.model_path, self.classifier_pickle), \"rb\"\n",
    "            )\n",
    "            self.classifier = pickle.load(load_model)\n",
    "\n",
    "            self.build_lookup_tables()\n",
    "\n",
    "            self.sentences_extractor()\n",
    "\n",
    "            self.is_ready()\n",
    "\n",
    "            pass\n",
    "\n",
    "        return None\n",
    "\n",
    "    def check_model(self):\n",
    "\n",
    "        \"\"\"Cerca se ci sono già i modelli per la comprensione del testo\"\"\"\n",
    "\n",
    "        risp = False\n",
    "\n",
    "        if self.classifier_pickle in os.listdir(self.model_path):\n",
    "            if self.label_encoder_pickle in os.listdir(self.model_path):\n",
    "                if self.crf_model_pickle in os.listdir(self.model_path):\n",
    "                    if self.vocabulary_pickle in os.listdir(self.model_path):\n",
    "                        risp = True\n",
    "                        \n",
    "        return risp\n",
    "\n",
    "    def get_status(self) -> str:\n",
    "\n",
    "        \"\"\"Restituisce lo stato attuale\"\"\"\n",
    "\n",
    "        return self.status\n",
    "\n",
    "    def is_ready(self) -> bool:\n",
    "\n",
    "        \"\"\"Verifica se l'istanza è pronta\"\"\"\n",
    "\n",
    "        if self.check_model():\n",
    "            self.status = \"ready\"\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def build_lookup_tables(self) -> bool:\n",
    "\n",
    "        df_entities = pd.read_excel(\"dataset.xlsx\", sheet_name=\"entities_slots\")\n",
    "\n",
    "        self.lookups = remove_nan(df_entities)\n",
    "\n",
    "        self.status = \"1 - lookups tables loaded\"\n",
    "\n",
    "        return True\n",
    "\n",
    "    def sentences_extractor(self) -> bool:\n",
    "\n",
    "        \"\"\"Estrae le frasi dell'utente (le salva in un file txt), del bot, della conversazione\"\"\"\n",
    "\n",
    "        # UTENTE\n",
    "\n",
    "        df_user = pd.read_excel(\"dataset.xlsx\", sheet_name=\"user\", header=None)\n",
    "        df_user.columns = [\"user\", \"sentences\"]\n",
    "        df_user[\"user\"] = df_user[\"user\"].fillna(method=\"ffill\", axis=0)\n",
    "\n",
    "        df_grouped = df_user.groupby(\"user\")\n",
    "\n",
    "        for group in df_grouped.groups:\n",
    "\n",
    "            self.user_sentences[group] = df_grouped.get_group(group)[\n",
    "                \"sentences\"\n",
    "            ].tolist()\n",
    "\n",
    "        # FILE CON LE FRASI\n",
    "\n",
    "#         i = True\n",
    "\n",
    "#         for k, v in self.user_sentences.items():\n",
    "\n",
    "#             for values in v:\n",
    "\n",
    "#                 if i:\n",
    "#                     with open(self.sentences_file, \"w+\") as f:\n",
    "#                         f.writelines(f\"{values}|{k}\\n\")\n",
    "#                         i = False\n",
    "#                 else:\n",
    "#                     with open(self.sentences_file, \"a+\") as f:\n",
    "#                         f.writelines(f\"{values}|{k}\\n\")\n",
    "\n",
    "        # BOT\n",
    "\n",
    "        df_bot = pd.read_excel(\"dataset.xlsx\", sheet_name=\"bot\", header=None)\n",
    "        df_bot.columns = [\"bot\", \"sentences\"]\n",
    "        df_bot[\"bot\"] = df_bot[\"bot\"].fillna(method=\"ffill\", axis=0)\n",
    "\n",
    "        df_grouped = df_bot.groupby(\"bot\")\n",
    "\n",
    "        for group in df_grouped.groups:\n",
    "\n",
    "            self.bot_sentences[group] = df_grouped.get_group(group)[\n",
    "                \"sentences\"\n",
    "            ].tolist()\n",
    "\n",
    "        # DIALOGS\n",
    "\n",
    "        df_dialogs = pd.read_excel(\"dataset.xlsx\", sheet_name=\"dialogs\")\n",
    "\n",
    "        self.dialogs = remove_nan(df_dialogs)\n",
    "\n",
    "        self.dialogs = {v[0]: v[1] for v in list(self.dialogs.values())}\n",
    "\n",
    "        self.df_query = pd.read_csv(self.db_file)\n",
    "\n",
    "        self.status = \"2 - estrazione dati completata\"\n",
    "\n",
    "        return True\n",
    "\n",
    "    def sentences_generator(self) -> bool:\n",
    "\n",
    "#         sentences, categories = [], []\n",
    "\n",
    "#         with open(self.sentences_file, encoding=\"utf-8\") as f:\n",
    "#             dataset = f.read()\n",
    "#             dataset = dataset.split(\"\\n\")\n",
    "\n",
    "#         for data in dataset:\n",
    "#             sentence = data.split(\"|\")\n",
    "\n",
    "#             if len(sentence) > 1:\n",
    "\n",
    "#                 # TODO: Lasciare upper ?\n",
    "\n",
    "#                 sentences.append(sentence[0].upper())\n",
    "#                 categories.append(sentence[1])\n",
    "\n",
    "#         assert len(sentences) == len(categories)\n",
    "\n",
    "        slots = list(lookups.keys())\n",
    "\n",
    "        for i in range(n_sentences):\n",
    "\n",
    "            category = random.choice(list(sentences.keys()))\n",
    "\n",
    "            sentence = random.choice(list(sentences[category]))\n",
    "            \n",
    "            sentence = sentence.upper()\n",
    "\n",
    "            regex_str = r\"\\[(?P<name>[a-zA-Z_]+)\\]\\((?P<value>[a-zA-Z\\' ]+)\\)+\"\n",
    "            slot_match = re.compile(regex_str)\n",
    "\n",
    "            matches = slot_match.findall(sentence)\n",
    "\n",
    "            dct = {k: v for k, v in matches}\n",
    "\n",
    "            for k, v in dct.items():\n",
    "\n",
    "                v2 = random.choice(lookups[k.upper()])\n",
    "\n",
    "                sentence = re.sub(f\"\\[{k}\\]\\({v}\\)\", f\"[{k}]({v2})\", sentence)\n",
    "\n",
    "            #     #     index = random.randint(0, len(sentences) - 1)\n",
    "\n",
    "            #     #     sentence = sentences[index]\n",
    "            #     #     category = categories[index]\n",
    "\n",
    "            #     for key in slots:\n",
    "\n",
    "            #         \"\"\"\n",
    "            #         Ogni volta che regex individua lo slot nella frase\n",
    "            #             sostituisce il valore con uno estratto in modo casuale\n",
    "            #         \"\"\"\n",
    "\n",
    "            #         #         regex_str = fr\"\\[{key}\\]\\((?P<value>[a-z ]+)\\)+\"\n",
    "\n",
    "            #         #         slot_match = re.compile(regex_str)\n",
    "\n",
    "            #         slot_sub = re.compile(fr\"\\[{key}\\]\\(.+\\)\")\n",
    "\n",
    "            #         repl = fr\"[{key}]({random.choice(lookups[key])})\"\n",
    "\n",
    "            #         sentence = slot_sub.sub(repl, sentence)\n",
    "\n",
    "            #         \"\"\"\n",
    "            #         Poi riassocia la categoria di partenza\n",
    "            #         \"\"\"\n",
    "\n",
    "            if i == 0:\n",
    "\n",
    "                with open(sentences_file_generated, \"w+\") as f:\n",
    "                    f.writelines(f\"{sentence}|{category}\\n\")\n",
    "\n",
    "            else:\n",
    "\n",
    "                with open(sentences_file_generated, \"a+\") as f:\n",
    "                    f.writelines(f\"{sentence}|{category}\\n\")\n",
    "\n",
    "        self.status = \"3 - frasi generate\"\n",
    "\n",
    "        return True\n",
    "\n",
    "    def prepare_data_crf(self) -> bool:\n",
    "\n",
    "        with open(self.sentences_file_generated, encoding=\"utf-8\") as f:\n",
    "            sentences = f.read()\n",
    "            sentences = sentences.split(\"\\n\")\n",
    "\n",
    "        arr_sentences = list()\n",
    "        arr_categories = list()\n",
    "\n",
    "        for n, sentence in enumerate(sentences):\n",
    "\n",
    "            try:\n",
    "\n",
    "                sentence, categ = sentence.split(\"|\")\n",
    "\n",
    "                sentence = rimuovi_punteggiatura(sentence)\n",
    "\n",
    "                arr_categories.append(categ)\n",
    "\n",
    "                splits = slot_match.split(sentence)\n",
    "\n",
    "                #         match = slot_match.search(frase)\n",
    "                matches = slot_match.findall(sentence)\n",
    "\n",
    "                dct = {k: v for k, v in matches}\n",
    "\n",
    "                if matches is not None:\n",
    "\n",
    "                    for split in splits:\n",
    "                        if split in list(dct.values()):\n",
    "                            for value in split.split():\n",
    "\n",
    "                                index = list(dct.values()).index(split)\n",
    "                                key = list(dct.keys())[index]\n",
    "\n",
    "                                arr_sentences.append([n, value, key])\n",
    "                        elif split in list(dct.keys()):\n",
    "                            pass\n",
    "                        else:\n",
    "                            for value in split.split():\n",
    "                                arr_sentences.append([n, value, \"O\"])\n",
    "\n",
    "                else:\n",
    "                    \"\"\"\n",
    "                    Serve per verificare se in qualche frase non avviene il match\n",
    "                    \"\"\"\n",
    "                    print(n, frase)\n",
    "\n",
    "            except Exception as err:\n",
    "                pass\n",
    "\n",
    "        df = pd.DataFrame(arr_sentences, columns=[\"n_frase\", \"word\", \"tag\"])\n",
    "        df_target = df[[\"n_frase\", \"tag\"]]\n",
    "\n",
    "        for k, v in df_target.groupby(\"n_frase\"):\n",
    "\n",
    "            self.y.append(v[\"tag\"].tolist())\n",
    "\n",
    "        df = extend_data(df, spacy=False)\n",
    "\n",
    "        for k, v in df.groupby(\"n_frase\"):\n",
    "\n",
    "            v.drop(columns=\"n_frase\", inplace=True)\n",
    "\n",
    "            self.X.append(v.to_dict(\"records\"))\n",
    "\n",
    "        self.status = \"4 - dati pronti per il training del crf\"\n",
    "\n",
    "        return True\n",
    "\n",
    "    #### INIZIO FUNZIONI PER IL CONDITIONAL RANDOM FIELD ####\n",
    "\n",
    "    def training_crf(self) -> bool:\n",
    "\n",
    "        self.crf.fit(self.X, self.y)\n",
    "\n",
    "        save_model = open(os.path.join(self.model_path, self.crf_model_pickle), \"wb\")\n",
    "        pickle.dump(self.crf, save_model)\n",
    "        save_model.close()\n",
    "\n",
    "        self.status = \"5 - crf model trained\"\n",
    "\n",
    "    def prepare_sentence(self, sentence: str) -> [pd.DataFrame, list]:\n",
    "\n",
    "        \"\"\"\n",
    "        Prepare la frase per il predict\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "\n",
    "        sentence : str\n",
    "\n",
    "            è la frase che sarà elaborata\n",
    "\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        df : DataFrame\n",
    "\n",
    "        X_arr[0] : array\n",
    "\n",
    "            dati in formato utile a CRF per il predict\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        sentence = rimuovi_punteggiatura(sentence)\n",
    "\n",
    "        X_arr = list()\n",
    "\n",
    "        df = pd.DataFrame(data=[i for i in sentence.split()], columns=[\"word\"])\n",
    "        df[\"n_frase\"] = 1\n",
    "\n",
    "        df = extend_data(df)\n",
    "\n",
    "        for k, v in df.groupby(\"n_frase\"):\n",
    "\n",
    "            v.drop(columns=\"n_frase\", inplace=True)\n",
    "\n",
    "            X_arr.append(v.to_dict(\"records\"))\n",
    "\n",
    "        return df, X_arr[0]\n",
    "\n",
    "    def extend_sentence(self, sentence: str) -> pd.DataFrame:\n",
    "\n",
    "        \"\"\"\n",
    "        Estrae slots e lo aggiunge al DataFrame come colonna\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "\n",
    "        sentence : str\n",
    "\n",
    "            è la stringa che contiene la frase\n",
    "\n",
    "        model : sklearn_crfsuite.estimator.CRF\n",
    "\n",
    "            è il modello addestrato di ConditionalRandomField\n",
    "\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        df : DataFrame\n",
    "\n",
    "            al DataFrame di partenza viene aggiunta una colonna\n",
    "            con l'indicazione del tipo di slot individuato\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        df, X_arr = self.prepare_sentence(sentence)\n",
    "\n",
    "        df[\"slots\"] = self.crf.predict_single(X_arr)\n",
    "\n",
    "        return df\n",
    "\n",
    "    #### DUCKLING ####\n",
    "\n",
    "    def extract_datetime(self, text: str, url=\"http://0.0.0.0:8000/parse\"):\n",
    "\n",
    "        data = {\"locale\": \"it_IT\", \"text\": text}\n",
    "\n",
    "        resp = None\n",
    "\n",
    "        datetimes = list()\n",
    "\n",
    "        try:\n",
    "\n",
    "            response = requests.post(url, data=data)\n",
    "\n",
    "            try:\n",
    "\n",
    "                if response.status_code == 200:\n",
    "\n",
    "                    for dt in response.json():\n",
    "\n",
    "                        if dt[\"dim\"] == \"time\":\n",
    "\n",
    "                            dtime = dt[\"value\"][\"value\"]\n",
    "                            dtime = dateparser.parse(dtime)\n",
    "\n",
    "                            datetimes.append(dtime)\n",
    "\n",
    "                resp = dict()\n",
    "\n",
    "                if len(datetimes) > 1:\n",
    "                    resp[\"datetime\"] = list([min(datetimes), max(datetimes)])\n",
    "                else:\n",
    "                    resp[\"datetime\"] = list(datetimes)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return resp\n",
    "\n",
    "    def slots_extractor(self, sentence: str) -> defaultdict:\n",
    "\n",
    "        \"\"\"\n",
    "        Restituisce un dictionary degli slots individuati\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "\n",
    "        sentence : str\n",
    "\n",
    "            è la stringa che contiene la frase\n",
    "\n",
    "        model : sklearn_crfsuite.estimator.CRF\n",
    "\n",
    "            è il modello addestrato di ConditionalRandomField\n",
    "\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        dd : dictionary\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        df = self.extend_sentence(sentence)\n",
    "\n",
    "        dd = defaultdict(list)\n",
    "\n",
    "        for k, v in df.query(\"slots != 'O'\").groupby(\"slots\"):\n",
    "            dd[k] = \" \".join(v[\"word\"])\n",
    "\n",
    "        \"\"\"\n",
    "        Estrazione date ed orari tramite duckling    \n",
    "        \"\"\"\n",
    "\n",
    "        date_time = self.extract_datetime(sentence)\n",
    "\n",
    "        if date_time is not None and len(date_time) > 0:\n",
    "            dd[\"DATETIMES\"] = date_time[\"datetime\"]\n",
    "\n",
    "        return dd\n",
    "\n",
    "    #### FINE FUNZIONI PER IL CONDITIONAL RANDOM FIELD ####\n",
    "\n",
    "    #### INIZIO FUNZIONI PER LA CLASSIFICAZIONE DELL'INTENT ####\n",
    "\n",
    "    def replace_slot_values(self, sentence_dict: dict) -> dict:\n",
    "\n",
    "        \"\"\"\n",
    "        Integrazione dictionary - parte 1 di 2\n",
    "\n",
    "\n",
    "        Per operare una riduzione delle variabili\n",
    "        sostituisce il valore con il relativo slot\n",
    "        \"\"\"\n",
    "\n",
    "        sentence_dict[\"replaced_sentence\"] = sentence_dict[\"sentence\"]\n",
    "\n",
    "        for k, v in sentence_dict[\"slots\"].items():\n",
    "\n",
    "            if k != \"DATETIMES\":\n",
    "\n",
    "                sentence_dict[\"replaced_sentence\"] = re.sub(\n",
    "                    v, k, sentence_dict[\"replaced_sentence\"]\n",
    "                )\n",
    "\n",
    "        return sentence_dict\n",
    "\n",
    "    def add_slots(self, sentence: str) -> dict:\n",
    "\n",
    "        \"\"\"\n",
    "        Integrazione dictionary - parte 2 di 2\n",
    "        \"\"\"\n",
    "\n",
    "        sentence_dict = {}\n",
    "\n",
    "        sentence = rimuovi_punteggiatura(sentence)\n",
    "\n",
    "        slots_dict = self.slots_extractor(sentence)\n",
    "\n",
    "        sentence_dict[\"sentence\"] = sentence\n",
    "        sentence_dict[\"slots\"] = slots_dict\n",
    "\n",
    "        sentence_dict = self.replace_slot_values(sentence_dict)\n",
    "\n",
    "        #     date_time = extract_datetime(sentence)\n",
    "\n",
    "        #     if date_time is not None and len(date_time) > 0:\n",
    "        #         sentence_dict['datetimes'] = date_time['datetime']\n",
    "        #     else:\n",
    "        #         sentence_dict['datetimes'] = False\n",
    "\n",
    "        return sentence_dict\n",
    "\n",
    "    def training_intents(self) -> bool:\n",
    "\n",
    "        \"\"\"Addestramento algoritmo di classificazione intents\"\"\"\n",
    "\n",
    "        df = pd.read_csv(self.sentences_file_generated, sep=\"|\", header=None)\n",
    "\n",
    "        df.columns = [\"sentences\", \"intents\"]\n",
    "\n",
    "        df[\"sentences\"] = df[\"sentences\"].apply(conserva_solo_slot_name)\n",
    "\n",
    "        self.cv.fit(df.sentences)\n",
    "\n",
    "        with open(\n",
    "            os.path.join(self.model_path, self.vocabulary_pickle), \"wb\"\n",
    "        ) as vocabs:\n",
    "            pickle.dump(self.cv, vocabs)\n",
    "\n",
    "        #### GENERAZIONE FRASI FAKE ####\n",
    "\n",
    "        fake_list = list()\n",
    "\n",
    "        for i in range(30):\n",
    "            fake_list.append(\n",
    "                \" \".join(random.choices(list(self.cv.vocabulary_.keys()), k=10))\n",
    "            )\n",
    "\n",
    "        df_fake = pd.DataFrame({\"sentences\": fake_list, \"intents\": \"fake\"})\n",
    "\n",
    "        df = pd.concat([df, df_fake], axis=0, ignore_index=True)\n",
    "\n",
    "        #### LABEL ENCODER ####\n",
    "\n",
    "        self.le.fit(df.intents.unique())\n",
    "\n",
    "        with open(\n",
    "            os.path.join(self.model_path, self.label_encoder_pickle), \"wb\"\n",
    "        ) as label_encoder:\n",
    "            pickle.dump(self.le, label_encoder)\n",
    "\n",
    "        labels_categories = self.le.transform(df.intents.values)\n",
    "\n",
    "        self.classifier.fit(\n",
    "            X=self.cv.transform(df[\"sentences\"].values).toarray(), y=labels_categories\n",
    "        )\n",
    "\n",
    "        with open(os.path.join(self.model_path, self.classifier_pickle), \"wb\") as cls:\n",
    "            pickle.dump(self.classifier, cls)\n",
    "\n",
    "        predicted_categories = [\n",
    "            self.classifier.predict(\n",
    "                [np.max(self.cv.transform(sentence.split()).toarray(), axis=0)]\n",
    "            )[0]\n",
    "            for sentence in df.sentences.values\n",
    "        ]\n",
    "\n",
    "        self.cm = confusion_matrix(labels_categories, predicted_categories)\n",
    "\n",
    "        self.status = \"6 - classifier model trained\"\n",
    "\n",
    "        return True\n",
    "\n",
    "    #### FINE FUNZIONI PER LA CLASSIFICAZIONE DELL'INTENT ####\n",
    "\n",
    "    def get_intents_and_slots(self, sentence: str, threasold=0.25) -> dict:\n",
    "\n",
    "        \"\"\"\n",
    "        Estrae gli intents e gli slots dalla frase\n",
    "        \"\"\"\n",
    "\n",
    "        sentence_dict = self.add_slots(sentence)\n",
    "\n",
    "        sentence = sentence_dict[\"replaced_sentence\"]\n",
    "\n",
    "        arr = self.cv.transform(\n",
    "            self.add_slots(sentence)[\"replaced_sentence\"].split()\n",
    "        ).toarray()\n",
    "\n",
    "        arr = np.max(arr, axis=0)\n",
    "\n",
    "        probs = self.classifier.predict_proba([arr])[0]\n",
    "\n",
    "        df = pd.DataFrame({\"classes\": self.le.classes_, \"probs\": probs})\n",
    "        df.sort_values(\"probs\", ascending=False, inplace=True)\n",
    "        classes_with_prob = df[df[\"probs\"] > threasold].to_dict(\"records\")\n",
    "        sentence_dict[\"intents\"] = classes_with_prob\n",
    "\n",
    "        max_intent = self.le.classes_[np.argmax(probs)]\n",
    "        sentence_dict[\"max_intent\"] = max_intent\n",
    "\n",
    "        return sentence_dict\n",
    "\n",
    "    def bot_reply(self, sentence: str, threasold=0.25) -> dict:\n",
    "\n",
    "        \"\"\"\n",
    "        Genera la risposta\n",
    "        \"\"\"\n",
    "\n",
    "        sentence = sentence.upper()\n",
    "\n",
    "        data = self.get_intents_and_slots(sentence, threasold)\n",
    "\n",
    "        if (data[\"intents\"][0][\"probs\"] > 0.75) and data[\"intents\"][0][\n",
    "            \"classes\"\n",
    "        ] != \"fake\":\n",
    "\n",
    "            intent = data[\"max_intent\"]\n",
    "\n",
    "            reply = self.dialogs[intent]\n",
    "\n",
    "            reply_str = random.choice(self.bot_sentences[reply])\n",
    "\n",
    "            query_list = list()\n",
    "\n",
    "            for k, v in data[\"slots\"].items():\n",
    "\n",
    "                if k != \"DATETIMES\":\n",
    "\n",
    "                    query_list.append(f\"{k} == '{v}'\")\n",
    "\n",
    "                    k = list(data[\"slots\"].keys())[0]\n",
    "                    k = f\"[{k}]\"\n",
    "\n",
    "                    v = list(data[\"slots\"].values())[0]\n",
    "\n",
    "                    reply_str = re.sub(f\"[{k}+]\", v, reply_str)\n",
    "\n",
    "            query = \" and \".join(query_list)\n",
    "\n",
    "            n = self.df_query.query(query)[\"NOME\"].drop_duplicates().count()\n",
    "\n",
    "            return reply_str % (n)\n",
    "\n",
    "        else:\n",
    "\n",
    "            intent = \"Non ho capito, riformula meglio la tua domanda\"\n",
    "\n",
    "            return intent\n",
    "\n",
    "    def training(self):\n",
    "\n",
    "        self.build_lookup_tables()\n",
    "\n",
    "        self.sentences_extractor()\n",
    "\n",
    "        self.sentences_generator()\n",
    "\n",
    "        self.prepare_data_crf()\n",
    "\n",
    "        self.training_crf()\n",
    "\n",
    "        self.training_intents()\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b79d2105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"botAI = BotAI(db_file=\\\"db_esempio.csv\\\")\";\n",
       "                var nbb_formatted_code = \"botAI = BotAI(db_file=\\\"db_esempio.csv\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# botAI = BotAI(db_file=\"db_esempio.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee716cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 29;\n",
       "                var nbb_unformatted_code = \"# botAI.get_status()\";\n",
       "                var nbb_formatted_code = \"# botAI.get_status()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# botAI.get_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52d9e9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 28;\n",
       "                var nbb_unformatted_code = \"# botAI.training_intents()\";\n",
       "                var nbb_formatted_code = \"# botAI.training_intents()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# botAI.training_intents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8980a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 27;\n",
       "                var nbb_unformatted_code = \"# botAI.cm\";\n",
       "                var nbb_formatted_code = \"# botAI.cm\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# botAI.cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c09b1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 26;\n",
       "                var nbb_unformatted_code = \"# print(f\\\"Accuracy: {sum(botAI.cm.diagonal()) / botAI.cm.sum() * 100}%\\\")\";\n",
       "                var nbb_formatted_code = \"# print(f\\\"Accuracy: {sum(botAI.cm.diagonal()) / botAI.cm.sum() * 100}%\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(f\"Accuracy: {sum(botAI.cm.diagonal()) / botAI.cm.sum() * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53511eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71193516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 25;\n",
       "                var nbb_unformatted_code = \"# botAI.bot_reply(\\\"quante fattorie didattiche ci sono a Caserta ?\\\")\";\n",
       "                var nbb_formatted_code = \"# botAI.bot_reply(\\\"quante fattorie didattiche ci sono a Caserta ?\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# botAI.bot_reply(\"quante fattorie didattiche ci sono a Caserta ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e23d75d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1ca8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
